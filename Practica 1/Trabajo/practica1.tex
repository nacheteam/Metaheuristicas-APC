\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%--------------------------------------------------------%
%         Paquetes usados sólo para esta entrega         %
%--------------------------------------------------------%

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}



\author{Ignacio Aguilera Martos \\
	DNI: 77448262V       e-mail: nacheteam@correo.ugr.es \\
	Grupo de prácticas 1 Lunes 17:30-19:30}
\title{Práctica 1 Búsqueda Local APC \\ Metaheurísticas \\ Algoritmos KNN, Relief y Búsqueda Local}
\date{Curso 2017-2018}

%Quita la sangría
\setlength{\parindent}{0cm}


\begin{document}
	\maketitle

	\tableofcontents

	\newpage

	%p 56

%	\framebox[16cm][c]{\LaTeX}

	\section{Introducción del problema}
	\label{sec:introProblema}

	Para el problema de clasificación partimos de un conjunto de datos dado por una serie de tuplas que contienen los valores de atributos para cada instancia. Esto es una n-tupla de valores reales en nuestro caso.

	El objetivo del problema es obtener un vector de pesos que asocia un valor en el intervalo $[0,1]$ indicativo de la relevancia de ese atributo. Esta relevancia va referida a lo importante que es en nuestro algoritmo clasificador ese atributo a la hora de computar la distancia entre elementos.

	Resumiendo lo que tenemos es un algoritmo clasificador que utiliza el vector de pesos calculado para predecir la clase a la que pertenece una instancia dada. Este algoritmo clasificador es el KNN con k=1. Lo que hace es calcular según la distancia euclídea (o cualquier otra) la tupla más cercana a la que queremos clasificar ponderando cada atributo con el correspondiente peso del vector, es decir, la distancia entre dos elementos sería:
	$$d(e,f) = \sqrt{\sum_{i=0}^{n}w_i * (e_{i} - f_{i})}$$
	Donde e y f son instancias del conjunto de datos, w el vector de pesos y n la longitud de e y f que es la misma.

	La calificación que se le asigna al vector w depende de dos cosas: la tasa de aciertos y la simplicidad.

	La tasa de aciertos se mide contando el número de aciertos al emplear el clasificador descrito y la simplicidad se mide como el número de elementos del vector de pesos que son menores que 0.2, ya que estos pesos no son empleados por el clasificador, o lo que es lo mismo, son sustituidos por cero. Por lo tanto las calificaciones siguen las fórmulas:
	$$Tasa\_acierto = 100\cdot \frac{nº  \ aciertos}{nº \ datos} \ , \ Tasa\_simplicidad = 100\cdot \frac{nº \ valores \ de \ w \ < \ 0.2}{nº \ de \ atributos}$$
	$$Tasa\_agregada = \frac{1}{2}\cdot Tasa\_acierto + \frac{1}{2}\cdot Tasa\_simplicidad$$
	Cabe destacar que todas las tasas están expresadas en porcentajes, por lo tanto cuanto más cercano sea el valor a 100 mejor es la calificación.

	De esta forma a través del algoritmo que obtiene el vector de pesos para el conjunto de datos dado y el clasificador obtenemos un programa que clasifica de forma automática las nuevas instancias de datos que se introduzcan.

	\newpage

	\section{Introducción de la práctica}
	\label{sec:introPractica}

	En esta práctica he analizado el comportamiento de los algoritmos KNN con k=1, el algoritmo greedy Relief y una implementación del algoritmo de búsqueda local para el problema de obtención de un vector de pesos para clasificar un conjunto de datos. Así mismo he realizado la implementación del algoritmo KNN con k variable para poder estudiar si varían los resultados al aumentar el valor de K o por contra obtenemos demasiado ajuste.

	Para empezar al leer los ficheros de datos dados para el problema me he dado cuenta de que tenemos tuplas repetidas, cosa que he tenido en cuenta para no usarlas en la clasificación, ya que siempre obtendríamos distancia 0 para dicha tupla. Para ello en vez de comprobar el índice dentro del vector he comprobado si las tuplas son iguales para no usarlas.

	Así mismo he implementado diferentes distancias a parte de la euclídea para comprobar si los resultados son mejores o peores en función de la distancia para cada conjunto de datos.

	Para terminar, antes de analizar los datos, se debe considerar que los datos han sido redondeados a 4 decimales para no obtener tablas excesivamente largas. Si se desea obtener los datos completos se puede ejecutar el programa como se describe en la sección \hyperref[sec:procedimiento]{\ref{sec:procedimiento}}.


	\section{Descripción común a todos los algoritmos}
	Los algoritmos empleados han sido el KNN, el algoritmo greedy Relief y la metaheurística de búsqueda local.

	Estos algoritmos comparten ciertos métodos y operadores que pasaré a explicar en esta sección.

	Para empezar se debe destacar que la representación escogida para las soluciones es un vector de números reales, es decir, si n es el número de características:
	$$w\in \mathbb{R}^n \ t.q. \ \forall i \ con \ 0\leq i < n \ se \ tiene \ w_i \in [0,1]$$
	O lo que es lo mismo, un vector de tamaño n con todas las posiciones rellenas con números del intervalo [0,1].

	A estos números me referiré como pesos asociados a las características, ya que lo que nos indican es el grado de importancia de dicha característica a la hora de clasificar los datos, siendo 1 el máximo de relevancia y 0 el mínimo.

	Así mismo cabe destacar que nuestra intención en este problema es obtener una buena calificación de dicho vector de pesos. Esto lo medimos mediante las tasas de acierto y simplicidad que se definen como:
	$$Tasa\_acierto = 100\cdot \frac{nº  \ aciertos}{nº \ datos} \ , \ Tasa\_simplicidad = 100\cdot \frac{nº \ valores \ de \ w \ < \ 0.2}{nº \ de \ atributos}$$
	$$Tasa\_agregada = \frac{1}{2}\cdot Tasa\_acierto + \frac{1}{2}\cdot Tasa\_simplicidad$$

	La tasa de aciertos lo que nos mide es en un porcentaje cuántas instancias hemos clasificado correctamente mediante el algoritmo KNN usando el vector de pesos w.

	La tasa de simplicidad nos mide cuántos de los valores que tiene el vector de pesos son menores que 0.2. Esto se hace ya que como imposición del problema tenemos que si alguna de los pesos es menor que 0.2 no debemos usarlo, o lo que es lo mismo, debemos sustituirlo por un 0 en la función de la distancia que luego describiré. Midiendo esto obtenemos un dato de cuanto sobreajuste ha tenido nuestro algoritmo a la hora de obtener el vector de pesos. Cuantas menos características necesitemos para discernir la clase a la que pertenece una instancia de los datos, más simple será clasificar dicha instancia. Se expresa en porcentaje indicando 0 como ninguna simplicidad y 100 como la máxima simplicidad.

	De esta forma combinando ambas tasas obtenemos la tasa agregada que nos hace la media entre ambas tasas, de forma que le asignamos la misma importancia a acertar en la clasificación de las instancias y a la simplicidad en la solución. Cabe destacar que es imposible obtener una tasa de un 100\% a no ser que los datos se compongan únicamente de un punto ya que ello implicaría que la simplicidad ha de ser un 100\% (todos las posiciones del vector menores que 0.2) y por tanto la distancia sería 0 en todos los casos. De esta forma aspiraremos a una calificación lo mas alta posible pero teniendo en cuenta las restricciones de la función objetivo construida.

	Las funciones y operadores de uso común los he agrupado en un fichero llamado auxiliar.py. Este fichero contiene las funciones de lectura de datos, distancias, una función que devuelve el elemento más común de una lista, la norma euclídea y una función para dividir los datos en el número de particiones que queramos manteniendo el porcentaje de elementos de cada clase que había en el conjunto de datos original.

	\subsection{Función de lectura de datos}

	La función de lectura de datos recibe la ruta del fichero arff y lee el contenido del mismo dando como resultado una lista de listas en la que cada una de ellas es una tupla o instancia de los datos.

	El pseudocódigo de la función es:

	\begin{algorithm}
		\caption{lecturaDatos(nombre\_fich)}
		\label{algoritmoLecturaDatos}
		\begin{algorithmic}
			\STATE data $\leftarrow$ [ ]
			\FOR{linea de nombre\_fich}
				\IF{se ha leído @data}
					\STATE data $\leftarrow$ [data,linea]
				\ENDIF
			\ENDFOR
			\FOR{tupla en data}
				\FOR{atributo en tupla}
				\STATE Normalizar.
				\ENDFOR
			\ENDFOR
			\RETURN data
		\end{algorithmic}
	\end{algorithm}

	Para esta implementación en concreto nos apoyamos en que Python tiene polimorfismo para todos los tipos de datos sin necesidad de declarar las variables, de forma que no nos importa que los datos sean numéricos o de tipo string.

	\subsection{Funciones de distancia}

	Las funciones de distancia siguen todas el mismo esquema de código, cambiando únicamente la fórmula empleada en cada caso para computar la distancia. Voy a describir las 3 distancias que he implementado teniendo esto en cuenta.

	Se debe tener en cuenta que e1 y e2 son ambos dos tuplas del conjunto de datos de las que vamos a obtener la distancia y w es el vector de pesos que toma parte en el cómputo.

	\begin{algorithm}
		\caption{distanciaEuclidea(e1,e2,w)}
		\begin{algorithmic}
			\IF{longitud(e1)!=longitud(e2)}
				\STATE No se puede hallar la distancia.
			\ELSE
				\STATE distancia = $\sqrt{\sum_{i=1}^{longitud(e1)}w_i\cdot (e1_i - e2_i)^2}$
			\ENDIF
		\end{algorithmic}
	\end{algorithm}

	El resto de funciones de distancia siguen el mismo esquema pero cambiando la fórmula para obtener el valor de la misma.

	\subsection{Función MasComun}

	Esta función lo que hace es devolvernos el elemento que más se repite dentro de un vector, esto es utilizado en el algoritmo KNN para obtener la clase más común entre los k elementos con distancia mínima al dado.

	\begin{algorithm}
		\caption{masComun(lista)}
		\begin{algorithmic}
			\STATE vector\_repeticiones $\leftarrow$ [ ]
			\FOR{elemento en lista}
				\STATE Contar el número de veces que aparece e introducirlo en el vector de repeticiones.
			\ENDFOR
			\STATE Obtener el elemento con mayor número de apariciones.
		\end{algorithmic}
	\end{algorithm}

	\subsection{Función de norma euclídea}

	Esta función toma una lista de elementos y devuelve la norma euclídea asociada al vector que representa la lista.

	\begin{algorithm}
		\caption{normaEuclidea(e)}
		\begin{algorithmic}
			\STATE norma $\leftarrow$ 0
			\FOR{ei en e}
				\STATE norma $\leftarrow$ norma+$ei^2$
			\ENDFOR
			\STATE norma $\leftarrow$ $\sqrt{norma}$
			\RETURN norma
		\end{algorithmic}
	\end{algorithm}

	\subsection{Función de división de datos}

	Esta función divide el conjunto de datos inicial en n particiones todas ellas respetando el porcentaje de ocurrencias de cada clase que tenía el conjunto de datos inicial.

	\begin{algorithm}
		\caption{divideDatosFCV(data,n)}
		\begin{algorithmic}
			\STATE particiones $\leftarrow$ [ ]
			\STATE tam\_particion $\leftarrow$ $\frac{longitud(data)}{n}$
			\STATE clases $\leftarrow$ Posibles clases del conjunto data.
			\STATE proporciones\_clases $\leftarrow$ [ ]
			\FOR{clase en clases}
				\STATE proporciones\_clases $\leftarrow$ [proporciones\_clases, nueva\_proporcion]
			\ENDFOR
			\FOR{i en 0..n}
				\STATE particion $\leftarrow$ [ ]
				\FOR{j en 0..numero\_clases}
					\STATE numero\_elementos\_clase = proporciones\_clases[j]*tam\_particion
					\FOR{k en 0..longitud(data)}
						\STATE Introducir en particion el número de elementos de clase calculado para cada clase.
					\ENDFOR
				\ENDFOR
			\ENDFOR
			\STATE Si han sobrado datos sin colocar los ponemos en la última partición.
			\RETURN particiones
		\end{algorithmic}
	\end{algorithm}
	
	\newpage

	\section{KNN}
	\label{sec:knn}

	El algoritmo KNN ha sido utilizado en estas prácticas tanto como clasificador (y por tanto con el objetivo de obtener una valoración de un vector de pesos) como algoritmo de comparación del algoritmo de búsqueda local.
	El guión de prácticas nos pedía implementar el algoritmo 1NN, pero yo he decidido implementarlo de forma genérica para poder comprobar cómo cambiaban los resultados cuando incrementamos el número de vecinos más cercanos.
	
	Este algoritmo toma como entrada un vector de pesos, un conjunto de datos de entrenamiento, otro de test y un valor k que nos indica cuántos vecinos más cercanos queremos tomar para clasificar cada dato. 
	El procedimiento consiste en tomar cada dato del conjunto de test y hallar los k datos del conjunto de entrenamiento que tienen distancia mínima con el dato tomado del conjunto de test. De ahí lo que hacemos es clasificar el dato con la clase más frecuente de entre los k vecinos más cercanos calculados. De esta forma, intuitivamente, asignamos la misma clase a datos que están próximos entre sí. Una vez obtenida la clase de estos elementos lo que hacemos es comprobar cuántas clases hemos acertado al clasificar de esta forma. Con esto obtenemos tras la ejecución del algoritmo un número entre 0 y 100 que nos indica cual ha sido nuestra tasa de acierto (100 todo correcto, 0 ninguna correcta).
	
	Para emplear este algoritmo como comparación para los otros lo que hacemos es introducir como vector de pesos un vector con todas las posiciones a 1. De esta forma obtenemos la calificación que obtendríamos si no hubiéramos incluido en la fórmula de la distancia una ponderación con pesos.
	
	El pseudocódigo del algoritmo es:
	\begin{algorithm}
		\caption{KNN(w,datos\_entrenamiento, datos\_test, k)}
		\begin{algorithmic}
			\STATE clases $\leftarrow$ [ ]
			\FOR{i en [0..longitud(datos\_test)-1]}
				\STATE distancias $\leftarrow$ [ ]
				\STATE minimos $\leftarrow$ [ ]
				\FOR{j en [0..longitud(datos\_entrenamiento)-1]}
					\IF{datos\_test[i]!=datos\_entrenamiento[j]}
						\STATE distancias $\leftarrow$ [distancias,[j,distancia(datos\_entrenamiento[j], datos\_test[i],w)]]
					\ENDIF
					\STATE Ordenar la lista de distancias según los valores de distancias.
					\STATE mínimos $\leftarrow$ los k primeros índices de la lista de distancias (distancias[i][0])
					\STATE clases\_minimos $\leftarrow$ [ ]
					\FOR{m en minimos}
						\STATE clases\_minimos $\leftarrow$ [clases\_minimos, datos\_entrenamiento[m][longitud(datos\_entrenamiento[m]-1)]]
					\STATE clases $\leftarrow$ [clases, masComun(clases\_minimos)]
					\ENDFOR
				\ENDFOR
			\ENDFOR
			\STATE bien\_clasificadas $\leftarrow$ 0
			\FOR{c en clases, d en datos\_test}
				\IF{c==d[longitud(d)-1]}
					\STATE bien\_clasificadas $\leftarrow$ bien\_clasificadas+1
				\ENDIF
			\ENDFOR
			\RETURN $\frac{bien\_clasificadas}{longitud(datos\_entrenamiento)}$
		\end{algorithmic}
	\end{algorithm}
	
	Cabe notar que el número que devolvemos está entre 0 y 1, por lo que en los algoritmos de valoración debemos tener esto en cuenta para multiplicarlo por 100 y convertirlo en un porcentaje.

	\section{Relief}
	\label{sec:relief}

	Este algoritmo es el que vamos a hacer competir con el de búsqueda local para comprobar cuál es el que nos da mejores resultados en términos generales. El algoritmo Relief es un algoritmo greedy que busca rapidez contra mejores resultados de clasificación.
	
	El algoritmo toma como entrada únicamente el conjunto de datos. Para cada elemento del conjunto buscamos el elemento de la misma clase más cercano y el elemento de la clase contraria más cercano de forma que tenemos al amigo y enemigos más cercanos. En la literatura es común encontrar estos elementos como near hit y near miss.
	
	Partiendo de un vector de pesos con todas las posiciones a 0 tomamos el amigo y enemigo más cercano y restamos a nuestro elemento del conjunto de datos tanto el amigo como el enemigo. Después de esto lo que hacemos es sumar la resta del enemigo y restar la resta del amigo, de forma que aumentamos la distancia que dicho vector de pesos produce para hacer que el amigo esté más cercano del dato que el enemigo.
	
	Esto ocurre ya que sumamos la distancia del elemento al enemigo haciendo que los pesos sean mayores para el mismo y restamos la distancia del elemento al amigo haciendo que nos acerque en distancia a dicha tupla.
	
	A continuación se describe el funcionamiento en pseudocódigo:
	
	\begin{algorithm}
		\caption{elementoMinimaDistancia(e,lista)}
		\begin{algorithmic}
			\STATE distancias $\leftarrow$ [ ]
			\FOR{l en lista}
				\IF{l!=e}
					\STATE distancias $\leftarrow$ [distancias, distancia(e,l,[1..1])]
				\ELSE
					\STATE distancias $\leftarrow$ [distancias, max(distancias)]
				\ENDIF
			\ENDFOR
			\STATE indice\_menor\_distancia $\leftarrow$ índice del elemento de menor valor del vector distancias.
			\RETURN lista[indice\_menor\_distancia]
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Relief(data)}
		\begin{algorithmic}
			\STATE w $\leftarrow$ vector de pesos a 0
			\FOR{elemento en data}
				\STATE clase $\leftarrow$ clase de elemento
				\STATE amigos $\leftarrow$ [ ]
				\STATE enemigos $\leftarrow$ [ ]
				\FOR{e en data}
					\IF{e!=elemento AND e[longitud(e)-1]==clase}
						\STATE amigos $\leftarrow$ [amigos, e]
					\ELSE
						\STATE enemigos $\leftarrow$ [enemigos, e]
					\ENDIF
				\ENDFOR
				\STATE amigo\_cercano $\leftarrow$ elementoMinimaDistancia(elemento, amigos)
				\STATE enemigo\_cercano $\leftarrow$ elementoMinimaDistancia(elemento, enemigos)
				\STATE resta\_enemigo $\leftarrow$ element-enemigo\_cercano
				\STATE resta\_amigo $\leftarrow$ element-amigo\_cercano
				\STATE w $\leftarrow$ w + resta\_enemigo - resta\_amigo
				\STATE $w_{max}$ $\leftarrow$ máximo de w
				\FOR{i en [0..longitud(w)-1]}
					\IF{w[i]<0}
						\STATE w[i] $\leftarrow$ 0
					\ELSE
						\STATE w[i] $\leftarrow$ $\frac{w[i]}{w_{max}}$
					\ENDIF
				\ENDFOR
			\ENDFOR
			\RETURN w
		\end{algorithmic}
	\end{algorithm}
	
	Tenemos en primer lugar la función elementoMinimaDistancia que nos devuelve el elemento de la lista más cercano al elemento e. De esta forma podemos hallar el amigo y enemigo más cercano para el algoritmo Relief. Cabe destacar que para que el vector de pesos esté con números entre 0 y 1 tenemos que poner en cada iteración del algoritmo los elementos del vector negativos a 0 y el resto los normalizamos dividiendo por el máximo elemento del vector de forma que todas las posiciones nos queden entre 0 y 1.
	
	\newpage

	\section{Búsqueda Local}
	\label{sec:bl}

	El algoritmo de búsqueda local tiene como intención, al igual que el anterior, proporcionarnos un vector de pesos que maximice la tasa agregada que obtenemos como ya he mencionado anteriormente.
	
	Este algoritmo comienza con un vector de pesos aleatorio generado con una distribución uniforme en el intervalo [0,1], obteniendo a partir de ahí mejores vectores a cada iteración. La generación de vecinos viene dada por un operador que toma posiciones aleatorias del vector de pesos (sin repetición) y les suma un valor aleatorio generado con una distribución normal de media 0 y desviación 0.3. Tras esto hacemos igual que en el algoritmo Relief para normalizar el vector de pesos y que cumpla las restricciones del problema.
	
	Dado el vector de pesos inicial obtenemos su valoración de la tasa agregada (incluye la simplicidad y la tasa de aciertos). Generamos un vecino mediante la forma mencionada anteriormente y comprobamos si su tasa es mayor que la del vector de pesos actual. Si esto es así descartamos nuestro vector de pesos y tomamos al vecino, en caso contrario continuaremos explorando el vecindario hasta encontrar uno que mejore el resultado, agotar las posibilidades de vecinos o llegar a 15.000 evaluaciones de la función objetivo o $20\cdot n$ vecinos explorados donde n es el número de atributos de cada tupla de datos.
	
	De esta manera otorgamos una forma de parar a nuestro algoritmo y no excederse en la búsqueda.
	
	Las funciones que intervienen, en pseudocódigo, son las siguientes:
	
	\begin{algorithm}
		\caption{mutacion(w,vector\_posiciones)}
		\begin{algorithmic}
			\STATE incremento $\leftarrow$ random.gauss(0,0.3)
			\STATE i $\leftarrow$ random.int(0,longitud(vector\_posiciones)-1)
			\STATE vector\_posiciones\_aux $\leftarrow$ [ ]
			\STATE pos $\leftarrow$ vector\_posiciones[i]
			\STATE w[pos] $\leftarrow$ w[pos]+incremento
			\STATE $w_{max}$ $\leftarrow$ maximo(w)
			\FOR{i en [0..longitud(w)-1]}
				\IF{w[i]<0}
					\STATE w[i] $\leftarrow$ 0
				\ELSE
					\STATE w[i] $\leftarrow$ $\frac{w[i]}{w_{max}}$
				\ENDIF
			\ENDFOR
			\FOR{v en vector\_posiciones}
				\IF{v!=pos}
					\STATE vector\_posiciones\_aux $\leftarrow$ [vector\_posiciones\_aux, v]
				\ENDIF
			\ENDFOR
			\RETURN w,vector\_posiciones\_aux
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{primerVector(n)}
		\begin{algorithmic}
			\STATE w $\leftarrow$ [ ]
			\FOR{i en [0..n-1]}
				\STATE w $\leftarrow$ [w, random.uniforme(0,1)]
			\ENDFOR
			\RETURN w
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{busquedaLocal(data,k)}
		\begin{algorithmic}
			\STATE MAX\_EVALUACIONES $\leftarrow$ 15000
			\STATE MAX\_VECINOS $\leftarrow$ $20\cdot longitud(data[0])$
			\STATE vecinos $\leftarrow$ 0
			\STATE evaluaciones $\leftarrow$ 0
			\STATE w $\leftarrow$ primerVector(longitud(data[0]))
			\STATE vector\_posiciones $\leftarrow$ [0..longitud(w)-1]
			\STATE valoracion\_actual $\leftarrow$ Valoracion(data,data,k,w)
			\WHILE{evaluaciones<MAX\_EVALUACIONES AND vecinos<MAX\_VECINOS}
				\STATE evaluaciones $\leftarrow$ evaluaciones+1
				\STATE vecinos $\leftarrow$ vecinos+1
				\STATE vecino, vector\_posiciones $\leftarrow$ mutacion(w,vector\_posiciones)
				\STATE valoracion\_vecino $\leftarrow$ Valoracion(data,data,k,vecino)
				\IF{valoracion\_vecino>valoracion\_actual}
					\STATE w $\leftarrow$ vecino
					\STATE valoracion\_actual $\leftarrow$ valoracion\_vecino
					\STATE vector\_posiciones $\leftarrow$ [0..longitud(w)-1]
				\ELSIF{vector\_posiciones==[ ]}
					\RETURN w
				\ENDIF
			\ENDWHILE
			\RETURN w
		\end{algorithmic}
	\end{algorithm}
	
	Debe tomarse en cuenta que la función Valoracion(data,data,k,w) devuelve la tasa agregada teniendo en cuenta la simplicidad y la tasa de acierto como se ha descrito en el algoritmo KNN.
	Este algoritmo, como ya discutiré en la parte de resultados, puede quedarse atrapado fácilmente en un máximo local y va a consumir mucho más tiempo, ya que cada llamada de la función valoración ejecuta el algoritmo KNN sobre dicho conjunto de datos sumando así mayor tiempo de ejecución que el de sus competidores.

	\newpage
	
	\section{Procedimiento de desarrollo de la práctica}
	\label{sec:procedimiento}
	
	El desarrollo de esta práctica lo he realizado en Python 3.5 implementando desde cero todos los métodos necesarios por mi cuenta. Para implementar los tres algoritmos principales de la práctica me he valido de las explicaciones del seminario 2, completando la información para entender el algoritmo (si era necesario) mediante internet. En cualquier caso los algoritmos implementados reflejan los esquemas del seminario 2 y no otros.
		
	Los archivos proporcionados de código son auxiliar.py donde se implementan los algoritmos y funciones comunes que se utilizan en todos o varios algoritmos con la intención de reutilizar las mismas. Así mismo el algoritmo KNN tiene su propio fichero donde está implementado y de igual forma lo tienen el algoritmo Relief y de búsqueda local. En cada uno de estos ficheros se incluye el algoritmo descrito en las secciones previas así como una función de valoración que se encarga de ejecutar el algoritmo para obtener el vector de pesos con 5 particiones y obtener las calificaciones obtenidas para cada una de las 5 particiones. De esta forma es más sencillo diseñar un script como el que se encuentra en el archivo resultados.py que toma dichas funciones y ejecuta para k=1,3,5 los algoritmos sobre los conjuntos de prueba. De esta forma obtenemos de manera automatizada todos los resultados que vemos en las tablas de la sección siguiente.
		
	Si se desea ejecutar el programa se recomienda compilar el código para que su ejecución sea mucho más rapida con el comando 'python3.5 -m compileall .'. Tras esto para ejecutar el script de resultados sólo tenemos que hacerlo como 'python3.5 resultados.py'.

	\section{Resultados}
	\label{sec:resultados}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 71.8750 & 0.0000 & 35.9375 & 0.4403 & 76.3158 & 0.0000 & 38.1579 & 0.0546 & 70.5882 & 0.0000 & 35.2941 & 0.2561 \\ [0.5ex] \hline
				Partición 2 & 84.3750 & 0.0000 & 42.1875 & 0.4341 & 81.5789 & 0.0000 & 40.7895 & 0.0527 & 77.9412 & 0.0000 & 38.9706 & 0.2959 \\ [0.5ex] \hline
				Partición 3 & 71.8750 & 0.0000 & 35.9375 & 0.4329 & 94.7368 & 0.0000 & 47.3684 & 0.0543 & 67.6471 & 0.0000 & 33.8235 & 0.3161 \\ [0.5ex] \hline
				Partición 4 & 81.2500 & 0.0000 & 40.6250 & 0.4340 & 73.6842 & 0.0000 & 36.8421 & 0.0530 & 60.2941 & 0.0000 & 30.1471 & 0.2984 \\ [0.5ex] \hline
				Partición 5 & 85.9375 & 0.0000 & 42.9688 & 0.4484 & 76.7442 & 0.0000 & 38.3721 & 0.0586 & 66.2338 & 0.0000 & 33.1169 & 0.2715 \\ [0.5ex] \hline
				Media & 79.0625 & 0.0000 & 39.5313 & 0.4380 & 80.6120 & 0.0000 & 40.3060 & 0.0546 & 68.5409 & 0.0000 & 34.2704 & 0.2876 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tabla1NN}
		\caption{Resultados 1NN}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 73.4375 & 0.0000 & 36.7188 & 0.4694 & 78.9474 & 0.0000 & 39.4737 & 0.0551 & 70.5882 & 0.0000 & 35.2941 & 0.2815 \\ [0.5ex] \hline
				Partición 2 & 85.9375 & 0.0000 & 42.9688 & 0.4559 & 76.3158 & 0.0000 & 38.1579 & 0.0555 & 75.0000 & 0.0000 & 37.5000 & 0.3794 \\ [0.5ex] \hline
				Partición 3 & 78.1250 & 0.0000 & 39.0625 & 0.4344 & 94.7368 & 0.0000 & 47.3684 & 0.0553 & 64.7059 & 0.0000 & 32.3529 & 0.3513 \\ [0.5ex] \hline
				Partición 4 & 82.8125 & 0.0000 & 41.4063 & 0.4433 & 71.0526 & 0.0000 & 35.5263 & 0.0557 & 70.5882 & 0.0000 & 35.2941 & 0.3800 \\ [0.5ex] \hline
				Partición 5 & 87.5000 & 0.0000 & 43.7500 & 0.4433 & 72.0930 & 0.0000 & 36.0565 & 0.0610 & 64.9351 & 0.0000 & 32.4975 & 0.3168 \\ [0.5ex] \hline
				Media & 81.5625 & 0.0000 & 40.7813 & 0.4493 & 78.6291 & 0.0000 & 39.3146 & 0.0565 & 69.1635 & 0.0000 & 34.5817 & 0.3418 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tabla3NN}
		\caption{Resultados 3NN}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 73.4375 & 0.0000 & 36.7188 & 0.5142 & 76.3158 & 0.0000 & 38.1579 & 0.0679 & 72.0588 & 0.0000 & 36.0294 & 0.3407 \\ [0.5ex] \hline
				Partición 2 & 90.6250 & 0.0000 & 45.3125 & 0.5176 & 81.5789 & 0.0000 & 40.7895 & 0.0587 & 72.0588 & 0.0000 & 36.0294 & 0.4101 \\ [0.5ex] \hline
				Partición 3 & 75.0000 & 0.0000 & 37.5000 & 0.5100 & 89.4737 & 0.0000 & 44.7368 & 0.0615 & 67.6471 & 0.0000 & 33.8235 & 0.3748 \\ [0.5ex] \hline
				Partición 4 & 81.2500 & 0.0000 & 40.6250 & 0.5261 & 76.3158 & 0.0000 & 38.1579 & 0.0592 & 69.1176 & 0.0000 & 34.5588 & 0.3774 \\ [0.5ex] \hline
				Partición 5 & 84.3750 & 0.0000 & 42.1875 & 0.4988 & 72.0930 & 0.0000 & 36.0465 & 0.0648 & 68.8312 & 0.00000 & 34.4156 & 0.3210 \\ [0.5ex] \hline
				Media & 80.9375 & 0.0000 & 40.4688 & 0.5133 & 79.1554 & 0.0000 & 39.5777 & 0.0624 & 69.9427 & 0.0000 & 34.9714 & 0.3648 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tabla5NN}
		\caption{Resultados 5NN}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 71.8750 & 98.6301 & 85.2526 & 0.9703 & 78.9474 & 47.8261 & 63.3867 & 0.1254 & 75.0000 & 46.6667 & 60.8333 & 0.4568 \\ [0.5ex] \hline
				Partición 2 & 87.5000 & 97.2603 & 92.3801 & 1.0143 & 81.5789 & 26.0869 & 53.8330 & 0.1267 & 73.5294 & 53.3333 & 63.4314 & 0.6407 \\ [0.5ex] \hline
				Partición 3 & 71.8750 & 97.2603 & 84.5676 & 1.0385 & 86.8421 & 47.8261 & 67.3341 & 0.1263 & 67.6471 & 60.0000 & 63.8235 & 0.7070 \\ [0.5ex] \hline
				Partición 4 & 81.2500 & 95.8904 & 88.5702 & 1.0530 & 78.9474 & 52.1739 & 65.5606 & 0.1254 & 72.0588 & 31.1111 & 51.5850 & 0.6594 \\ [0.5ex] \hline
				Partición 5 & 85.9375 & 98.6301 & 92.2838 & 0.9963 & 72.0930 & 43.4783 & 57.7856 & 0.1180 & 72.7273 & 48.8889 & 60.8081 & 0.3986 \\ [0.5ex] \hline
				Media & 79.6875 & 97.5342 & 88.6109 & 1.0145 & 79.6818 & 43.4783 & 61.5800 & 0.1244 & 72.1925 & 48.0000 & 60.0963 & 0.5725 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaReliefK1}
		\caption{Resultados Relief con K=1}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 73.4375 & 98.6301 & 86.0338 & 0.9413 & 78.9474 & 47.8261 & 63.3867 & 0.1317 & 66.1765 & 46.6667 & 56.4216 & 0.5166 \\ [0.5ex] \hline
				Partición 2 & 85.9375 & 97.2603 & 91.5989 & 0.9480 & 78.9474 & 26.0870 & 52.5172 & 0.1304 & 80.8824 & 53.3333 & 67.1078 & 0.7211 \\ [0.5ex] \hline
				Partición 3 & 78.1250 & 97.2603 & 87.6926 & 0.9244 & 94.7368 & 47.8261 & 71.2815 & 0.1322 & 58.8235 & 60.0000 & 59.4118 & 0.8060 \\ [0.5ex] \hline
				Partición 4 & 81.2500 & 95.8904 & 88.5702 & 0.9436 & 68.4211 & 52.1739 & 60.2975 & 0.1321 & 70.5882 & 31.1111 & 50.8497 & 0.6745 \\ [0.5ex] \hline
				Partición 5 & 87.5000 & 98.6301 & 93.0651 & 0.9589 & 72.0930 & 43.4783 & 57.7856 & 0.1242 & 71.4276 & 48.8889 & 60.1587 & 0.4445 \\ [0.5ex] \hline
				Media & 81.2500 & 97.5342 & 89.3921 & 0.9432 & 78.6291 & 43.4783 & 61.0537 & 0.1301 & 69.5798 & 48.0000 & 58.7899 & 0.6325 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaReliefK3}
		\caption{Resultados Relief con K=3}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 73.4375 & 98.6301 & 86.0338 & 1.0732 & 84.2105 & 47.8261 & 66.0183 & 0.1356 & 66.1765 & 46.6667 & 56.4216 & 0.5553 \\ [0.5ex] \hline
				Partición 2 & 89.0625 & 97.2603 & 93.1614 & 1.0930 & 78.9474 & 26.0870 & 52.5172 & 0.1364 & 77.9412 & 53.3333 & 65.6373 & 0.7449 \\ [0.5ex] \hline
				Partición 3 & 75.0000 & 97.2603 & 86.1301 & 1.0775 & 100.0000 & 47.8261 & 73.9130 & 0.1351 & 69.1176 & 60.0000 & 64.5588 & 0.8443 \\ [0.5ex] \hline
				Partición 4 & 81.2500 & 95.8904 & 88.5705 & 1.0991 & 76.3158 & 52.1739 & 64.2449 & 0.1383 & 69.1176 & 31.1111 & 50.1144 & 0.8855 \\ [0.5ex] \hline
				Partición 5 & 84.3750 & 98.6301 & 91.5026 & 1.0808 & 76.7442 & 43.4783 & 60.1112 & 0.1294 & 71.4286 & 48.8889 & 60.1587 & 0.5262 \\ [0.5ex] \hline
				Media & 80.6250 & 97.5342 & 89.0796 & 1.0847 & 83.2436 & 43.4783 & 63.3609 & 0.1350 & 70.7563 & 48.0000 & 59.3782 & 0.7112 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaReliefK5}
		\caption{Resultados Relief con K=5}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 71.8750 & 34.2466 & 53.0608 & 186.6400 & 78.9474 & 26.0870 & 52.5172 & 9.0305 & 70.5882 & 35.5556 & 53.0719 & 52.5939 \\ [0.5ex] \hline
				Partición 2 & 76.5625 & 54.7945 & 65.6785 & 436.2914 & 81.5789 & 39.1304 & 60.3547 & 6.8897 & 82.3529 & 24.4444 & 53.3987 & 92.8766 \\ [0.5ex] \hline
				Partición 3 & 70.3125 & 30.1370 & 50.2247 & 185.3929 & 86.8421 & 17.3913 & 52.1167 & 4.3802 & 70.5882 & 57.7778 & 64.1830 & 120.7886 \\ [0.5ex] \hline
				Partición 4 & 82.8125 & 54.7945 & 68.8035 & 354.7868 & 78.9474 & 52.1739 & 65.5606 & 21.2622 & 63.2353 & 31.1111 & 47.1732 & 70.7596 \\ [0.5ex] \hline
				Partición 5 & 84.3750 & 39.7260 & 62.0505 & 224.3229 & 74.4186 & 30.4348 & 52.4267 & 9.3806 & 64.9351 & 44.4444 & 54.6898 & 90.2908 \\ [0.5ex] \hline
				Media & 77.1875 & 42.7397 & 59.9636 & 277.4868 & 80.1469 & 33.0435 & 56.5952 & 10.1886 & 70.3400 & 38.6667 & 54.5033 & 85.4619 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaBLK1}
		\caption{Resultados Búsqueda Local con K=1}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 73.4375 & 46.5753 & 60.0064 & 248.5178 & 76.3158 & 34.7826 & 55.5492 & 18.8915 & 66.1765 & 46.6667 & 56.4216 & 124.4612 \\ [0.5ex] \hline
				Partición 2 & 85.9375 & 56.1644 & 71.0509 & 365.1825 & 78.9474 & 56.5217 & 67.7346 & 15.0689 & 75.0000 & 26.6667 & 50.8333 & 74.7928 \\ [0.5ex] \hline
				Partición 3 & 78.1250 & 52.0548 & 65.0899 & 267.7560 & 92.1053 & 26.0870 & 59.0961 & 8.4903 & 69.1176 & 26.6667 & 47.8922 & 69.5649 \\ [0.5ex] \hline
				Partición 4 & 79.6875 & 46.5753 & 63.1314 & 312.7265 & 68.4211 & 34.7826 & 51.6018 & 12.1651 & 70.5882 & 42.2222 & 56.4052 & 94.7792 \\ [0.5ex] \hline
				Partición 5 & 89.0625 & 46.5753 & 67.8189 & 329.6873 & 69.7674 & 17.3913 & 43.5794 & 6.3797 & 64.9351 & 40.0000 & 52.4675 & 41.5187 \\ [0.5ex] \hline
				Media & 81.2500 & 49.5890 & 65.4195 & 304.7740 & 77.1114 & 33.9130 & 55.5122 & 12.1991 & 69.1635 & 36.4444 & 52.8040 & 81.0234 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaBLK3}
		\caption{Resultados Búsqueda Local con K=3}
	\end{table}

	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				Partición 1 & 71.8750 & 45.2055 & 58.5402 & 201.1896 & 73.6842 & 52.1737 & 62.9291 & 16.7641 & 82.3529 & 48.8889 & 65.6209 & 172.0540 \\ [0.5ex] \hline
				Partición 2 & 89.0625 & 36.9863 & 63.0244 & 274.0396 & 81.5789 & 26.0870 & 53.8330 & 11.6506 & 79.4118 & 57.7778 & 68.5948 & 143.7965 \\ [0.5ex] \hline
				Partición 3 & 75.0000 & 34.2466 & 54.6233 & 192.7606 & 89.4737 & 34.7826 & 62.1281 & 20.1932 & 69.1176 & 57.7778 & 63.4477 & 183.0782 \\ [0.5ex] \hline
				Partición 4 & 84.3750 & 43.8356 & 64.1053 & 363.2454 & 73.6842 & 43.4783 & 58.5812 & 11.8747 & 60.2941 & 53.3333 & 56.8137 & 199.8569 \\ [0.5ex] \hline
				Partición 5 & 89.0625 & 61.6438 & 75.3532 & 593.0892 & 72.0930 & 47.8261 & 59.9596 & 13.0924 & 67.5325 & 60.0000 & 63.7662 & 115.5035 \\ [0.5ex] \hline
				Media & 81.8750 & 44.3836 & 63.1293 & 324.8649 & 78.1028 & 40.8696 & 59.4862 & 14.7150 & 71.7418 & 55.5556 & 63.6487 & 162.8578 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaBLK5}
		\caption{Resultados Búsqueda Local con K=5}
	\end{table}
	
	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				1-NN & 79.0625 & 0.0000 & 39.5313 & 0.4380 & 80.6120 & 0.0000 & 40.3060 & 0.0546 & 68.5409 & 0.0000 & 34.2704 & 0.2876 \\ [0.5ex] \hline
				Relief & 79.6875 & 97.5342 & 88.6109 & 1.0145 & 79.6818 & 43.4783 & 61.5800 & 0.1244 & 72.1925 & 48.0000 & 60.0963 & 0.5725 \\ [0.5ex] \hline
				BL & 77.1875 & 42.7397 & 59.9636 & 277.4868 & 80.1469 & 33.0435 & 56.5952 & 10.1886 & 70.3400 & 38.6667 & 54.5033 & 85.4619 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaGlobalK1}
		\caption{Resultados globales con K=1}
	\end{table}
	
	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				3-NN & 81.5625 & 0.0000 & 40.7813 & 0.4493 & 78.6291 & 0.0000 & 39.3146 & 0.0565 & 69.1635 & 0.0000 & 34.5817 & 0.3418 \\ [0.5ex] \hline
				Relief & 81.2500 & 97.5342 & 89.3921 & 0.9432 & 78.6291 & 43.4783 & 61.0537 & 0.1301 & 69.5798 & 48.0000 & 58.7899 & 0.6325 \\ [0.5ex] \hline
				BL & 81.2500 & 49.5890 & 65.4195 & 304.7740 & 77.1114 & 33.9130 & 55.5122 & 12.1991 & 69.1635 & 36.4444 & 52.8040 & 81.0234 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaGlobalK3}
		\caption{Resultados globales con K=3}
	\end{table}
	
	\begin{table}[ht]
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c |}
				\cline{2-13}
				\multicolumn{1}{c|}{} & \multicolumn{4}{|c|}{Ozone} & \multicolumn{4}{| c|}{Parkinsons} & \multicolumn{4}{|c|}{Spectf-Heart} \\ [0.5ex]
				\cline{2-13}
				\multicolumn{1}{c|}{} & \%\_clas & \%\_red & Agr. & T (seg) & \%\_clas & \%\_red & Agr. & T  (seg) & \%\_clas & \%\_red & Agr. & T  (seg) \\ [0.5ex] \hline
				5-NN & 80.9375 & 0.0000 & 40.4688 & 0.5133 & 79.1554 & 0.0000 & 39.5777 & 0.0624 & 69.9427 & 0.0000 & 34.9714 & 0.3648 \\ [0.5ex] \hline
				Relief & 80.6250 & 97.5342 & 89.0796 & 1.0847 & 83.2436 & 43.4783 & 63.3609 & 0.1350 & 70.7563 & 48.0000 & 59.3782 & 0.7112 \\ [0.5ex] \hline
				BL & 81.8750 & 44.3836 & 63.1293 & 324.8649 & 78.1028 & 40.8696 & 59.4862 & 14.7150 & 71.7418 & 55.5556 & 63.6487 & 162.8578 \\ [0.5ex] \hline
			\end{tabular}
		}
		\label{tablaGlobalK5}
		\caption{Resultados globales con K=5}
	\end{table}


\end{document}
